I have the following code where the values to be searched are hardcoded example "MARD_FSP" and "MARD_MDNA"
Now I want to make it Dynamic and the list of values is given by the below code
SELECT
    'MARD_' || SOL_PROD_TOKEN AS MARD_FILTER
FROM
    mard_mdna.v_all_solutions_mdna;


SELECT DISTINCT
    v_users.username AS User_Name,
    CASE 
        WHEN UPPER(v_sql.sql_text) LIKE '%MARD\_FSP.%' ESCAPE '\' THEN 'MARD_FSP'
        WHEN UPPER(v_sql.sql_text) LIKE '%MARD\_MDNA.%' ESCAPE '\' THEN 'MARD_MDNA'
        ELSE TRIM(REPLACE(REGEXP_SUBSTR(UPPER(v_sql.sql_text), '\[A-Z0-9\_\]+\.', 1, 1), '.', ''))
    END AS Schema_Name,
    v_hist.session_id AS Session_ID,
    v_hist.max_sample_time AS Time_Stamp,
    v_hist.sql_id AS SQL_Query_ID,
    SUBSTR(v_sql.sql_text, 1, 500) AS SQL_Preview,  -- Limited to 500 characters
    LENGTH(v_sql.sql_text) AS SQL_TEXT_LENGTH
FROM 
(
    SELECT 
        sql_id, 
        SQL_OPCODE, 
        user_id,
        session_id,
        MAX(sample_time) AS max_sample_time
    FROM gv$ACTIVE_SESSION_HISTORY
    WHERE sql_id IS NOT NULL
        AND SQL_OPCODE = 3
        AND sample_time > SYSDATE - 2/24  -- Last 2 hours
    GROUP BY
        sql_id, 
        SQL_OPCODE,
        user_id,
        session_id
) v_hist
INNER JOIN (
    SELECT user_id, username 
    FROM all_users
    WHERE username NOT LIKE 'BUSV%' 
        AND username NOT LIKE 'HRMZ%' 
        AND username NOT LIKE 'MARD%' 
        AND username NOT LIKE 'MARS%' 
        AND username NOT LIKE 'RAWB%' 
        AND username NOT LIKE 'RAWO%'  
        AND username NOT LIKE 'RAWV%' 
        AND username NOT LIKE 'REDL%' 
        AND username NOT LIKE 'REFM%'   
        AND username NOT LIKE 'STAG%' 
        AND username NOT LIKE 'SYS%'
) v_users ON v_hist.user_id = v_users.user_id
LEFT JOIN (     
    SELECT DISTINCT sql_id, sql_text
    FROM gv$sqlarea
    WHERE sql_id IS NOT NULL 
        AND COMMAND_TYPE = 3
        AND UPPER(sql_text) LIKE '%MARD\_FSP.%' ESCAPE '\'
) v_sql ON v_sql.sql_id = v_hist.sql_id
WHERE v_sql.sql_text IS NOT NULL
ORDER BY v_hist.max_sample_time DESC;


QUERYID: String
TIMESTAMP: TimeStamp
USER_ID: String
DATAMART: String
STATEMENT_LENGTH: Integer
STATEMENT_SAMPLE: String
EXECUTION_COUNT: Integer
PARSING_SCHEMA: String


I have a table in MS Fabric: 
Table Name: LH_CD_Datamarts.mard_click_dashboard.t_staging_datamart_users_claude
I want to analyze this table and then clean and store the data in a new table via MS Fabric PySpark Notebook
New Table name:LH_CD_Datamarts.mard_click_dashboard.t_datamart_user_metrics
Table names can change so make them as variables

Table Column Details: 
 |-- QUERYID: string (nullable = true)
 |-- TIMESTAMP: string (nullable = true)
 |-- USER_ID: string (nullable = true)
 |-- DATAMART: string (nullable = true)
 |-- STATEMENT_LENGTH: decimal(38,0) (nullable = true)
 |-- STATEMENT_SAMPLE: string (nullable = true)
 |-- EXECUTION_COUNT: decimal(38,0) (nullable = true)
 |-- PARSING_SCHEMA: string (nullable = true)

 The Primary key will be of the following columns: 
 TIMESTAMP, QUERYID, USER_ID, DATAMART

 I want first analyse that how many duplicates are there based on this combination.
 Load the data in a dataframe
 I want some stats that is percentage of duplicates total records etc
 Also it would be best while moving the data STATEMENT_LENGTH and EXECUTION_COUNT columns can be convereted to integers
 And then transfer the fresh data without duplicates in the new table. IF they are duplicates based on the above columns only take the first insatnce
 Ensure in the new dataframe TIMESTAMP, QUERYID, USER_ID, DATAMART these columns form the primary key
Now after cleaning the data need to do load incremental load 
 IF the table does not exist identify as initial load and then load the cleaned data
 else based on the primary keys TIMESTAMP, QUERYID, USER_ID, DATAMART only load the required data which does not exist in the target table
 and also stats about the table like now of rows and column data types.
 No of rows loaded, No of rows before loading, no of rows after loading


Now I want to do some analysis with the table using Pyspark generate me tables and charts
- Top 10 Datamarts used by User ID
- How many statements are there where STATEMENT_LENGTH is more than equal to 1000
- TOP 10 USER_ID + Datamarts COMBINATIONS along with SQL Statment count 
And other useful stats from these columns and the keyfigure will be count of Query_Ids

- 
 |-- QUERYID: string (nullable = true)
 |-- TIMESTAMP: string (nullable = true)
 |-- USER_ID: string (nullable = true)
 |-- DATAMART: string (nullable = true)
 |-- STATEMENT_LENGTH: integer (nullable = true)
 |-- STATEMENT_SAMPLE: string (nullable = true)