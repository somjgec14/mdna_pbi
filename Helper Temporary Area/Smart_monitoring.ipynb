{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f58a68",
   "metadata": {},
   "source": [
    "I am trying to do the following steps in pyspark notebook in MS Fabric: \n",
    "Step 1: I am doing the following text\n",
    "Step 2: Create Reference DF for Frequency\n",
    "Step 3: Determining Frequency for each Semantic_Model_ID\n",
    "Step 4: Save Spark DF to Delta Table\n",
    "CAn you please optimize the code below with debugging messages at proper sections.\n",
    "Generate and ipynb notebook for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb50e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99441bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header_Files\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from notebookutils import mssparkutils\n",
    "print(\"All Libraries Loaded\")\n",
    "\n",
    "# Get connections\n",
    "client_id = mssparkutils.credentials.getSecret(\"https://kv-pbi-techical-accounts.vault.azure.net/\", \"clientid\")\n",
    "client_secret = mssparkutils.credentials.getSecret(\"https://kv-pbi-techical-accounts.vault.azure.net/\", \"clientsecret\")\n",
    "tenant_id = mssparkutils.credentials.getSecret(\"https://kv-pbi-techical-accounts.vault.azure.net/\", \"tenantid\")\n",
    "dataset_id = mssparkutils.credentials.getSecret(\"https://kv-pbi-techical-accounts.vault.azure.net/\", \"datasetid-fabric-capacity-metrics\")\n",
    "resource = 'https://analysis.windows.net/powerbi/api'\n",
    "api_version = 'v1.0'\n",
    "refresh_schedule_path = 'abfss://14b44c0a-e3c3-41eb-b31f-c2a7d90ce593@onelake.dfs.fabric.microsoft.com/e6e52507-e786-469c-8048-c816d44f5b5a/Tables/mard_mdna_t_pbi_sm_refresh_schedule'\n",
    "\n",
    "# Get access token\n",
    "auth_url = 'https://login.microsoftonline.com/{0}/oauth2/token'.format(tenant_id)\n",
    "data = {\n",
    "    'grant_type': 'client_credentials',\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'resource': resource\n",
    "}\n",
    "auth_response = requests.post(auth_url, data=data)\n",
    "access_token = auth_response.json()['access_token']\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer {0}'.format(access_token)\n",
    "}\n",
    "\n",
    "# Step 1: Get Refresh Schedule delta table\n",
    "df_spark_refresh_schedule = spark.read.format(\"delta\").load(refresh_schedule_path)\n",
    "\n",
    "df_refresh_schedule = df_spark_refresh_schedule.toPandas()\n",
    "\n",
    "print(df_refresh_schedule.head())\n",
    "print(df_refresh_schedule.info())\n",
    "\n",
    "\n",
    "# Step 2: Create Reference DF for Frequency\n",
    "df_refresh_frequency = pd.DataFrame()\n",
    "df_refresh_frequency['Key_SM_Refresh_Day'] = None\n",
    "df_refresh_frequency['Semantic_Model_ID'] = None\n",
    "df_refresh_frequency['Refresh_Day'] = None\n",
    "\n",
    "df_refresh_frequency['Key_SM_Refresh_Day'] = df_refresh_schedule.apply(\n",
    "    lambda row: \n",
    "    row['Semantic_Model_ID'] + \"_\" \n",
    "    + str(row['Time_of_the_Day']) + \"_\"\n",
    "    + row['Time_zone'] \n",
    ", axis=1)\n",
    "\n",
    "df_refresh_frequency['Refresh_Day'] = df_refresh_schedule.apply(\n",
    "    lambda row: \n",
    "    row['Day']\n",
    ", axis=1)\n",
    "\n",
    "df_refresh_frequency['Semantic_Model_ID'] = df_refresh_schedule.apply(\n",
    "    lambda row: \n",
    "    row['Semantic_Model_ID']\n",
    ", axis=1)\n",
    "print(df_refresh_frequency.head())\n",
    "print(df_refresh_frequency.info())\n",
    "\n",
    "# Step 3: Determining Frequency for each Semantic_Model_ID\n",
    "##Loading the Keys in a List for iteration\n",
    "\n",
    "distinct_keys = df_refresh_frequency['Key_SM_Refresh_Day'].unique().tolist()\n",
    "print(f\"Length of Keys is: {len(distinct_keys)}\")\n",
    "list_frequency = []\n",
    "\n",
    "# Define the set of all days in a week\n",
    "all_days = {\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"}\n",
    "\n",
    "# Iterate over the distinct_keys list\n",
    "for key1 in distinct_keys:\n",
    "    # Filter the dataframe for rows where 'Key' matches the current key\n",
    "    df_temp = df_refresh_frequency[df_refresh_frequency['Key_SM_Refresh_Day'] == key1]\n",
    "    \n",
    "    # Get the unique Semantic_Model_ID from df_temp (assuming it's the same for all rows)\n",
    "    semantic_model_id = df_temp['Semantic_Model_ID'].iloc[0]\n",
    "    \n",
    "    # Get the unique days from the Refresh_Day column\n",
    "    refresh_days = set(df_temp['Refresh_Day'].unique())\n",
    "    \n",
    "    # Determine the frequency based on the days present\n",
    "    if refresh_days == all_days:\n",
    "        frequency = \"Daily\"\n",
    "    else:\n",
    "        frequency = \"Weekly\"\n",
    "    \n",
    "    # Append the result to the final_results list\n",
    "    list_frequency.append(\n",
    "        {\"Key_Frequency\": semantic_model_id + \"_\" + frequency, \n",
    "        \"Semantic_Model_ID\": semantic_model_id, \n",
    "        \"Frequency\": frequency}\n",
    "    )\n",
    "\n",
    "# Create the final dataframe from the results\n",
    "df_frequency = pd.DataFrame(list_frequency)\n",
    "\n",
    "# Remove duplicate rows from the final dataframe\n",
    "df_frequency = df_frequency.drop_duplicates()\n",
    "\n",
    "# Print the final dataframe\n",
    "print(df_frequency)\n",
    "\n",
    "# Inner Join with df_refresh_schedule\n",
    "df_merged = pd.merge(df_frequency, df_refresh_schedule, on='Semantic_Model_ID', how='inner')\n",
    "print(df_merged.info())\n",
    "\n",
    "# Step 4: Save Spark DF to Delta Table\n",
    "df_Key_destination = spark.sql(\"SELECT DISTINCT Key_Frequency FROM mdna_pbi_monitoring.MARD_MDNA_T_PBI_SM_Refresh_Schedule_Frequency\").collect()\n",
    "\n",
    "destination_keys = {row['Key_Frequency'] for row in df_Key_destination}\n",
    "\n",
    "loaded_rows = 0\n",
    "not_loaded_rows = 0\n",
    "filtered_rows = []\n",
    "unfiltered_rows = []\n",
    "\n",
    "# Compare Key_Frequency from df_merged with Key_Frequency from mdna_pbi_monitoring.MARD_MDNA_T_PBI_SM_Refresh_Schedule_Frequency Table\n",
    "for index, row in df_merged.iterrows():\n",
    "    df_Key = row['Key_Frequency']\n",
    "    if df_Key not in destination_keys:\n",
    "        filtered_rows.append(row)\n",
    "        loaded_rows += 1\n",
    "    else:\n",
    "        not_loaded_rows += 1\n",
    "        unfiltered_rows.append(row)\n",
    "\n",
    "print(\"No. of Loaded Rows: {0} \\nNo. of Not Loaded Rows: {1}\".format(loaded_rows, not_loaded_rows))\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_rows) # New records \n",
    "unfiltered_df = pd.DataFrame(unfiltered_rows) # Existing records\n",
    "\n",
    "\n",
    "# If new Data is there append to the the database\n",
    "if loaded_rows > 0: \n",
    "    df_spark_rowsfiltered = spark.createDataFrame(filtered_df)\n",
    "    df_spark_rowsfiltered.write.format(\"delta\").mode(\"append\").saveAsTable(\"MARD_MDNA_T_PBI_SM_Refresh_Schedule_Frequency\")\n",
    "\n",
    "# Post-load checking\n",
    "df_count = spark.sql(\"SELECT count(*) FROM mdna_pbi_monitoring.MARD_MDNA_T_PBI_SM_Refresh_Schedule_Frequency\")\n",
    "display(df_count)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
