{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power BI Semantic Model Refresh Schedule Frequency Analysis\n",
    "\n",
    "This notebook processes Power BI refresh schedule data to determine frequency patterns and stores the results in a Delta Lake table.\n",
    "\n",
    "## Process Overview:\n",
    "1. Load refresh schedule data from Delta table\n",
    "2. Create frequency reference DataFrame\n",
    "3. Determine semantic model frequencies (Daily/Weekly)\n",
    "4. Save processed data to Delta table\n",
    "\n",
    "## Author: Data Engineering Team\n",
    "## Last Modified: 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, array_contains, when, countDistinct, \n",
    "    collect_set, size, array_intersect, array, concat_ws\n",
    ")\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from notebookutils import mssparkutils\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize global variables for tracking\n",
    "total_execution_start_time = time.time()\n",
    "step_timings = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POWER BI SEMANTIC MODEL REFRESH SCHEDULE FREQUENCY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Notebook execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"All libraries loaded successfully.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Configuration and Connection Setup\n",
    "print(\"\\nüîß Starting Step 0: Configuration and Authentication Setup...\")\n",
    "step_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Retrieve secrets from Azure Key Vault\n",
    "    print(\"   üìã Retrieving connection secrets from Azure Key Vault...\")\n",
    "    client_id = mssparkutils.credentials.getSecret(\n",
    "        \"https://kv-pbi-techical-accounts.vault.azure.net/\", \"clientid\"\n",
    "    )\n",
    "    client_secret = mssparkutils.credentials.getSecret(\n",
    "        \"https://kv-pbi-techical-accounts.vault.azure.net/\", \"clientsecret\"\n",
    "    )\n",
    "    tenant_id = mssparkutils.credentials.getSecret(\n",
    "        \"https://kv-pbi-techical-accounts.vault.azure.net/\", \"tenantid\"\n",
    "    )\n",
    "    dataset_id = mssparkutils.credentials.getSecret(\n",
    "        \"https://kv-pbi-techical-accounts.vault.azure.net/\", \n",
    "        \"datasetid-fabric-capacity-metrics\"\n",
    "    )\n",
    "    \n",
    "    # Configuration variables\n",
    "    resource = 'https://analysis.windows.net/powerbi/api'\n",
    "    api_version = 'v1.0'\n",
    "    refresh_schedule_path = (\n",
    "        'abfss://14b44c0a-e3c3-41eb-b31f-c2a7d90ce593@onelake.dfs.fabric.microsoft.com/'\n",
    "        'e6e52507-e786-469c-8048-c816d44f5b5a/Tables/mard_mdna_t_pbi_sm_refresh_schedule'\n",
    "    )\n",
    "    delta_table_destination = \"mard_mdna_t_pbi_sm_refresh_schedule_frequency\"\n",
    "    delta_table_database = \"mdna_pbi_monitoring\"\n",
    "\n",
    "    print(\"   ‚úÖ Successfully retrieved all connection secrets.\")\n",
    "    print(f\"   üìä Target Database: {delta_table_database}\")\n",
    "    print(f\"   üìä Target Table: {delta_table_destination}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Error retrieving secrets: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    logger.error(error_msg)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0.1: Obtain Access Token\n",
    "print(\"\\nüîê Starting Step 0.1: Obtaining Power BI API access token...\")\n",
    "\n",
    "try:\n",
    "    auth_url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/token'\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "        'resource': resource\n",
    "    }\n",
    "    \n",
    "    auth_response = requests.post(auth_url, data=data, timeout=30)\n",
    "    auth_response.raise_for_status()\n",
    "    access_token = auth_response.json()['access_token']\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "    \n",
    "    print(\"   ‚úÖ Access token obtained successfully.\")\n",
    "    print(f\"   üïí Token acquired at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    error_msg = f\"‚ùå Error obtaining access token: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    logger.error(error_msg)\n",
    "    raise\n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Unexpected error during token acquisition: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    logger.error(error_msg)\n",
    "    raise\n",
    "\n",
    "step_end_time = time.time()\n",
    "step_timings['Step_0'] = step_end_time - step_start_time\n",
    "print(f\"\\n‚úÖ Step 0 completed successfully in {step_timings['Step_0']:.2f} seconds.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Refresh Schedule Data\n",
    "print(\"\\nüìä Starting Step 1: Loading Refresh Schedule data from Delta table...\")\n",
    "step_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    print(f\"   üìÅ Source Path: {refresh_schedule_path}\")\n",
    "    \n",
    "    # Load data from Delta table\n",
    "    df_spark_refresh_schedule = spark.read.format(\"delta\").load(refresh_schedule_path)\n",
    "    \n",
    "    # Cache the DataFrame for better performance in subsequent operations\n",
    "    df_spark_refresh_schedule.cache()\n",
    "    \n",
    "    # Check if DataFrame is empty\n",
    "    row_count = df_spark_refresh_schedule.count()\n",
    "    \n",
    "    if row_count == 0:\n",
    "        warning_msg = \"‚ö†Ô∏è  Warning: Refresh Schedule Delta table is empty. No data to process.\"\n",
    "        print(warning_msg)\n",
    "        logger.warning(warning_msg)\n",
    "        # You might want to exit here or handle empty data case\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Successfully loaded refresh schedule data.\")\n",
    "        print(f\"   üìà Total rows: {row_count:,}\")\n",
    "        \n",
    "        # Display schema information\n",
    "        print(\"\\n   üìã Schema of df_spark_refresh_schedule:\")\n",
    "        df_spark_refresh_schedule.printSchema()\n",
    "        \n",
    "        # Show sample data\n",
    "        print(\"\\n   üîç Sample data (first 5 rows):\")\n",
    "        df_spark_refresh_schedule.show(5, truncate=False)\n",
    "        \n",
    "        # Additional data quality checks\n",
    "        unique_semantic_models = df_spark_refresh_schedule.select(\"Semantic_Model_ID\").distinct().count()\n",
    "        print(f\"   üìä Unique Semantic Models: {unique_semantic_models:,}\")\n",
    "        \n",
    "        # Check for null values in critical columns\n",
    "        critical_columns = [\"Semantic_Model_ID\", \"Day\", \"Time_of_the_Day\", \"Time_zone\"]\n",
    "        null_counts = {}\n",
    "        for col_name in critical_columns:\n",
    "            if col_name in df_spark_refresh_schedule.columns:\n",
    "                null_count = df_spark_refresh_schedule.filter(col(col_name).isNull()).count()\n",
    "                null_counts[col_name] = null_count\n",
    "                if null_count > 0:\n",
    "                    print(f\"   ‚ö†Ô∏è  Warning: {null_count:,} null values found in column '{col_name}'\")\n",
    "        \n",
    "        if all(count == 0 for count in null_counts.values()):\n",
    "            print(\"   ‚úÖ Data quality check: No null values found in critical columns.\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Error loading refresh schedule data from Delta table: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    logger.error(error_msg)\n",
    "    raise\n",
    "\n",
    "step_end_time = time.time()\n",
    "step_timings['Step_1'] = step_end_time - step_start_time\n",
    "print(f\"\\n‚úÖ Step 1 completed successfully in {step_timings['Step_1']:.2f} seconds.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create Frequency Reference DataFrame\n",
    "print(\"\\nüîß Starting Step 2: Creating Frequency Reference DataFrame...\")\n",
    "step_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    print(\"   üîÑ Transforming data using optimized Spark DataFrame operations...\")\n",
    "    \n",
    "    # Create reference DataFrame using Spark operations (optimized)\n",
    "    df_refresh_frequency_spark = df_spark_refresh_schedule.withColumn(\n",
    "        \"Key_SM_Refresh_Day\",\n",
    "        concat_ws(\"_\", \n",
    "                 col(\"Semantic_Model_ID\"), \n",
    "                 col(\"Time_of_the_Day\").cast(\"string\"), \n",
    "                 col(\"Time_zone\"))\n",
    "    ).select(\n",
    "        \"Key_SM_Refresh_Day\",\n",
    "        col(\"Day\").alias(\"Refresh_Day\"),\n",
    "        \"Semantic_Model_ID\"\n",
    "    )\n",
    "    \n",
    "    # Cache for performance\n",
    "    df_refresh_frequency_spark.cache()\n",
    "    \n",
    "    # Validate the transformation\n",
    "    transformed_row_count = df_refresh_frequency_spark.count()\n",
    "    \n",
    "    if transformed_row_count == 0:\n",
    "        warning_msg = \"‚ö†Ô∏è  Warning: df_refresh_frequency_spark is empty after transformation.\"\n",
    "        print(warning_msg)\n",
    "        logger.warning(warning_msg)\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Successfully created frequency reference DataFrame.\")\n",
    "        print(f\"   üìà Total rows after transformation: {transformed_row_count:,}\")\n",
    "        \n",
    "        # Display schema\n",
    "        print(\"\\n   üìã Schema of df_refresh_frequency_spark:\")\n",
    "        df_refresh_frequency_spark.printSchema()\n",
    "        \n",
    "        # Show sample data\n",
    "        print(\"\\n   üîç Sample transformed data (first 5 rows):\")\n",
    "        df_refresh_frequency_spark.show(5, truncate=False)\n",
    "        \n",
    "        # Additional statistics\n",
    "        unique_keys = df_refresh_frequency_spark.select(\"Key_SM_Refresh_Day\").distinct().count()\n",
    "        unique_days = df_refresh_frequency_spark.select(\"Refresh_Day\").distinct().count()\n",
    "        \n",
    "        print(f\"   üìä Unique Key_SM_Refresh_Day entries: {unique_keys:,}\")\n",
    "        print(f\"   üìä Unique refresh days found: {unique_days}\")\n",
    "        \n",
    "        # Show distribution of refresh days\n",
    "        print(\"\\n   üìà Distribution of refresh days:\")\n",
    "        day_distribution = df_refresh_frequency_spark.groupBy(\"Refresh_Day\").count().orderBy(\"count\", ascending=False)\n",
    "        day_distribution.show()\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Error creating frequency reference DataFrame: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    logger.error(error_msg)\n",
    "    raise\n",
    "\n",
    "step_end_time = time.time()\n",
    "step_timings['Step_2'] = step_end_time - step_start_time\n",
    "print(f\"\\n‚úÖ Step 2 completed successfully in {step_timings['Step_2']:.2f} seconds.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Determine Frequency for each Semantic Model\n",
    "print(\"\\nüéØ Starting Step 3: Determining Frequency for each Semantic_Model_ID...\")\n",
    "step_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    print(\"   üîÑ Analyzing refresh patterns to determine frequency...\")\n",
    "    \n",
    "    # Define all days of the week for comparison\n",
    "    all_days_list = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "    print(f\"   üìÖ Reference days for daily frequency check: {all_days_list}\")\n",
    "\n",
    "    # Group by Semantic_Model_ID and collect distinct refresh days\n",
    "    print(\"   üìä Grouping by Semantic_Model_ID and collecting distinct refresh days...\")\n",
    "    df_grouped_days = df_refresh_frequency_spark.groupBy(\"Semantic_Model_ID\").agg(\n",
    "        collect_set(\"Refresh_Day\").alias(\"Refresh_Days_Set\")\n",
    "    )\n",
    "    \n",
    "    # Cache for performance\n",
    "    df_grouped_days.cache()\n",
    "    \n",
    "    grouped_count = df_grouped_days.count()\n",
    "    print(f\"   üìà Grouped {grouped_count:,} unique semantic models.\")\n",
    "\n",
    "    # Determine frequency based on collected days using optimized logic\n",
    "    print(\"   üîç Applying frequency determination logic...\")\n",
    "    df_frequency_spark = df_grouped_days.withColumn(\n",
    "        \"Refresh_Days_Count\",\n",
    "        size(col(\"Refresh_Days_Set\"))\n",
    "    ).withColumn(\n",
    "        \"Frequency\",\n",
    "        when(\n",
    "            size(array_intersect(\n",
    "                col(\"Refresh_Days_Set\"), \n",
    "                array(*[lit(d) for d in all_days_list])\n",
    "            )) == lit(7), \"Daily\"\n",
    "        ).otherwise(\"Weekly\")\n",
    "    ).withColumn(\n",
    "        \"Key_Frequency\",\n",
    "        concat_ws(\"_\", col(\"Semantic_Model_ID\"), col(\"Frequency\"))\n",
    "    ).select(\n",
    "        \"Key_Frequency\", \n",
    "        \"Semantic_Model_ID\", \n",
    "        \"Frequency\", \n",
    "        \"Refresh_Days_Set\",\n",
    "        \"Refresh_Days_Count\"\n",
    "    )\n",
    "    \n",
    "    # Cache for performance\n",
    "    df_frequency_spark.cache()\n",
    "    \n",
    "    frequency_count = df_frequency_spark.count()\n",
    "    \n",
    "    if frequency_count == 0:\n",
    "        warning_msg = \"‚ö†Ô∏è  Warning: df_frequency_spark is empty after frequency determination.\"\n",
    "        print(warning_msg)\n",
    "        logger.warning(warning_msg)\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Successfully determined frequencies.\")\n",
    "        print(f\"   üìà Total unique semantic models processed: {frequency_count:,}\")\n",
    "        \n",
    "        # Display schema\n",
    "        print(\"\\n   üìã Schema of df_frequency_spark:\")\n",
    "        df_frequency_spark.printSchema()\n",
    "        \n",
    "        # Show sample data\n",
    "        print(\"\\n   üîç Sample frequency data (first 5 rows):\")\n",
    "        df_frequency_spark.show(5, truncate=False)\n",
    "        \n",
    "        # Frequency distribution analysis\n",
    "        print(\"\\n   üìä Frequency Distribution Analysis:\")\n",
    "        frequency_distribution = df_frequency_spark.groupBy(\"Frequency\").count().orderBy(\"Frequency\")\n",
    "        frequency_distribution.show()\n",
    "        \n",
    "        # Collect distribution for summary\n",
    "        freq_summary = frequency_distribution.collect()\n",
    "        for row in freq_summary:\n",
    "            print(f\"       {row['Frequency']}: {row['count']:,} semantic models\")\n",
    "\n",
    "    # Perform inner join with original refresh schedule\n",
    "    print(\"\\n   üîó Performing inner join with original refresh schedule data...\")\n",
    "    df_merged_spark = df_spark_refresh_schedule.join(\n",
    "        df_frequency_spark.select(\"Semantic_Model_ID\", \"Key_Frequency\", \"Frequency\"),\n",
    "        on=\"Semantic_Model_ID\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Cache for performance\n",
    "    df_merged_spark.cache()\n",
    "    \n",
    "    merged_count = df_merged_spark.count()\n",
    "    \n",
    "    if merged_count == 0:\n",
    "        warning_msg = \"‚ö†Ô∏è  Warning: df_merged_spark is empty after the join operation.\"\n",
    "        print(warning_msg)\n",
    "        logger.warning(warning_msg)\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Successfully merged frequency data with refresh schedule.\")\n",
    "        print(f\"   üìà Total merged rows: {merged_count:,}\")\n",
    "        \n",
    "        # Display merged schema\n",
    "        print(\"\\n   üìã Schema of df_merged_spark:\")\n",
    "        df_merged_spark.printSchema()\n",
    "        \n",
    "        # Show sample merged data\n",
    "        print(\"\\n   üîç Sample merged data (first 3 rows):\")\n",
    "        df_merged_spark.show(3, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Error determining frequencies or merging DataFrames: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    logger.error(error_msg)\n",
    "    raise\n",
    "\n",
    "step_end_time = time.time()\n",
    "step_timings['Step_3'] = step_end_time - step_start_time\n",
    "print(f\"\\n‚úÖ Step 3 completed successfully in {step_timings['Step_3']:.2f} seconds.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save to Delta Table\n",
    "print(\"\\nüíæ Starting Step 4: Saving processed data to Delta Table...\")\n",
    "step_start_time = time.time()\n",
    "\n",
    "# Initialize variables for tracking\n",
    "loaded_rows_count = 0\n",
    "unfiltered_rows_count = 0\n",
    "total_records_after_load = 0\n",
    "\n",
    "try:\n",
    "    # Ensure target database exists\n",
    "    print(f\"   üèóÔ∏è  Ensuring database '{delta_table_database}' exists...\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {delta_table_database}\")\n",
    "    print(f\"   ‚úÖ Database '{delta_table_database}' is ready.\")\n",
    "    \n",
    "    # Check if target table exists\n",
    "    table_full_name = f\"{delta_table_database}.{delta_table_destination}\"\n",
    "    \n",
    "    try:\n",
    "        # Try to read existing table to check if it exists\n",
    "        existing_table_check = spark.sql(f\"SELECT COUNT(*) as count FROM {table_full_name} LIMIT 1\")\n",
    "        existing_table_check.collect()  # This will fail if table doesn't exist\n",
    "        table_exists = True\n",
    "        print(f\"   üìä Target table '{table_full_name}' exists.\")\n",
    "    except:\n",
    "        table_exists = False\n",
    "        print(f\"   üÜï Target table '{table_full_name}' does not exist - will be created.\")\n",
    "    \n",
    "    if table_exists:\n",
    "        # Read existing keys from Delta table for deduplication\n",
    "        print(\"   üîç Reading existing Key_Frequency values for deduplication...\")\n",
    "        existing_keys_df = spark.sql(f\"SELECT DISTINCT Key_Frequency FROM {table_full_name}\")\n",
    "        existing_keys = [row.Key_Frequency for row in existing_keys_df.collect()]\n",
    "        existing_keys_count = len(existing_keys)\n",
    "        print(f\"   üìà Found {existing_keys_count:,} distinct Key_Frequency values in existing table.\")\n",
    "        \n",
    "        # Filter for new records only\n",
    "        print(\"   üîÑ Filtering for new records only...\")\n",
    "        df_new_records = df_merged_spark.filter(~col(\"Key_Frequency\").isin(existing_keys))\n",
    "    else:\n",
    "        # If table doesn't exist, all records are new\n",
    "        print(\"   üìù Table doesn't exist - all records will be inserted.\")\n",
    "        df_new_records = df_merged_spark\n",
    "        existing_keys_count = 0\n",
    "    \n",
    "    # Cache new records DataFrame\n",
    "    df_new_records.cache()\n",
    "    \n",
    "    # Count new and existing records\n",
    "    loaded_rows_count = df_new_records.count()\n",
    "    total_merged_count = df_merged_spark.count()\n",
    "    unfiltered_rows_count = total_merged_count - loaded_rows_count\n",
    "    \n",
    "    print(f\"\\n   üìä Data Analysis Summary:\")\n",
    "    print(f\"       Total processed records: {total_merged_count:,}\")\n",
    "    print(f\"       New records to insert: {loaded_rows_count:,}\")\n",
    "    print(f\"       Existing records (skipped): {unfiltered_rows_count:,}\")\n",
    "    \n",
    "    if loaded_rows_count > 0:\n",
    "        print(f\"\\n   üíæ Inserting {loaded_rows_count:,} new records into '{table_full_name}'...\")\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df_final = df_new_records.withColumn(\n",
    "            \"load_timestamp\", \n",
    "            lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        )\n",
    "        \n",
    "        # Write to Delta table\n",
    "        write_start_time = time.time()\n",
    "        df_final.write.format(\"delta\").mode(\"append\").saveAsTable(table_full_name)\n",
    "        write_end_time = time.time()\n",
    "        \n",
    "        write_duration = write_end_time - write_start_time\n",
    "        print(f\"   ‚úÖ Successfully inserted new records in {write_duration:.2f} seconds.\")\n",
    "        print(f\"   üìä Write performance: {loaded_rows_count/write_duration:.0f} rows/second\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è  No new records to insert. Delta table remains unchanged.\")\n",
    "\n",
    "    # Post-load verification\n",
    "    print(\"\\n   üîç Performing post-load verification...\")\n",
    "    verification_start_time = time.time()\n",
    "    \n",
    "    df_count_after_load = spark.sql(f\"SELECT count(*) as total_count FROM {table_full_name}\")\n",
    "    total_records_after_load = df_count_after_load.collect()[0][0]\n",
    "    \n",
    "    verification_end_time = time.time()\n",
    "    print(f\"   üìä Total records in '{table_full_name}': {total_records_after_load:,}\")\n",
    "    print(f\"   ‚è±Ô∏è  Verification completed in {verification_end_time - verification_start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Display count result\n",
    "    print(\"\\n   üìã Final table statistics:\")\n",
    "    df_count_after_load.show()\n",
    "    \n",
    "    # Additional table statistics if records were inserted\n",
    "    if loaded_rows_count > 0:\n",
    "        print(\"   üìà Final frequency distribution in table:\")\n",
    "        final_frequency_dist = spark.sql(f\"\"\"\n",
    "            SELECT Frequency, COUNT(*) as count \n",
    "            FROM {table_full_name} \n",
    "            GROUP BY Frequency \n",
    "            ORDER BY Frequency\n",
    "        \"\"\")\n",
    "        final_frequency_dist.show()\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = f\"‚ùå Error saving data to Delta table: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    logger.error(error_msg)\n",
    "    \n",
    "    # Additional error context\n",
    "    print(f\"   üîç Error context:\")\n",
    "    print(f\"       Target database: {delta_table_database}\")\n",
    "    print(f\"       Target table: {delta_table_destination}\")\n",
    "    print(f\"       Full table name: {table_full_name}\")\n",
    "    raise\n",
    "\n",
    "step_end_time = time.time()\n",
    "step_timings['Step_4'] = step_end_time - step_start_time\n",
    "print(f\"\\n‚úÖ Step 4 completed successfully in {step_timings['Step_4']:.2f} seconds.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Cleanup\n",
    "total_execution_end_time = time.time()\n",
    "total_execution_time = total_execution_end_time - total_execution_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä EXECUTION SUMMARY:\")\n",
    "print(f\"   ‚è±Ô∏è  Total execution time: {total_execution_time:.2f} seconds ({total_execution_time/60:.1f} minutes)\")\n",
    "print(f\"   üìÖ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\nüìà PROCESSING STATISTICS:\")\n",
    "print(f\"   üìä Total new records inserted: {loaded_rows_count:,}\")\n",
    "print(f\"   üìä Total existing records (skipped): {unfiltered_rows_count:,}\")\n",
    "print(f\"   üìä Final table record count: {total_records_after_load:,}\")\n",
    "print(f\"   üìä Target table: {delta_table_database}.{delta_table_destination}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  STEP-BY-STEP TIMING BREAKDOWN:\")\n",
    "for step, duration in step_timings.items():\n",
    "    percentage = (duration / total_execution_time) * 100\n",
    "    print(f\"   {step}: {duration:.2f}s ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüßπ CLEANUP:\")\n",
    "# Unpersist cached DataFrames to free memory\n",
    "try:\n",
    "    if 'df_spark_refresh_schedule' in locals():\n",
    "        df_spark_refresh_schedule.unpersist()\n",
    "    if 'df_refresh_frequency_spark' in locals():\n",
    "        df_refresh_frequency_spark.unpersist()\n",
    "    if 'df_grouped_days' in locals():\n",
    "        df_grouped_days.unpersist()\n",
    "    if 'df_frequency_spark' in locals():\n",
    "        df_frequency_spark.unpersist()\n",
    "    if 'df_merged_spark' in locals():\n",
    "        df_merged_spark.unpersist()\n",
    "    if 'df_new_records' in locals():\n",
    "        df_new_records.unpersist()\n",
    "    print(\"   ‚úÖ Successfully released cached DataFrames from memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Note: Some DataFrames may not have been unpersisted: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüèÅ All processing steps completed successfully!\")\n",
    "print(f\"üíæ Data has been successfully saved to Delta table: {delta_table_database}.{delta_table_destination}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Log final summary\n",
    "logger.info(f\"Notebook completed successfully. Inserted {loaded_rows_count:,} new records. \"\n",
    "           f\"Total execution time: {total_execution_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}